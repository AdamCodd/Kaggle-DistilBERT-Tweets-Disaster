{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch_lightning\n!pip install transformers\n!pip install torchmetrics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-05T23:06:01.841608Z","iopub.execute_input":"2023-10-05T23:06:01.842343Z","iopub.status.idle":"2023-10-05T23:06:31.901238Z","shell.execute_reply.started":"2023-10-05T23:06:01.842308Z","shell.execute_reply":"2023-10-05T23:06:31.900045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nimport pandas as pd\nimport pytorch_lightning as pl\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW, Adam\nfrom torch.utils.data import DataLoader, Dataset\nfrom pytorch_lightning import seed_everything\nimport torch.nn.functional as F\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom torchmetrics.classification import F1Score, Accuracy\n\nrandom_seed = 1270\nseed_everything(random_seed)\n\nprint(f\"Random seed used: {random_seed}\")\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nMAX_LENGTH = 160\nTRAIN_BATCH_SIZE = 32\nTEST_BATCH_SIZE = 32\nNUM_EPOCHS = 2\nLEARNING_RATE = 3e-5\n\nprint(f\"LR: {LEARNING_RATE}\")\n\nprint(\"Loading data...\")\n# Read the datasets from their respective CSV files\ntrain_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\nprint('Training Set Shape = {}'.format(train_df.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(train_df.memory_usage().sum() / 1024**2))\nprint('Test Set Shape = {}'.format(test_df.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(test_df.memory_usage().sum() / 1024**2))\n\n# Split the original training data into a new training set and a validation set\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['target'])\n\ntrain_df[\"length\"] = train_df[\"text\"].apply(lambda x : len(x))\ntest_df[\"length\"] = test_df[\"text\"].apply(lambda x : len(x))\n\nprint(\"Train Length Stat\")\nprint(train_df[\"length\"].describe())\nprint()\n\nprint(\"Test Length Stat\")\nprint(test_df[\"length\"].describe())\nprint()\n\n# Custom Dataset class for DataLoader\nprint(\"Preparing custom dataset...\")\nclass DisasterTweetDataset(Dataset):\n    def __init__(self, df, tokenizer, is_test=False):\n        self.texts = df['text'].tolist()\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        if not is_test:\n            self.labels = df['target'].tolist()\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=MAX_LENGTH)\n        \n        if self.is_test:\n            return {\n                'input_ids': torch.tensor(encoding['input_ids']), \n                'attention_mask': torch.tensor(encoding['attention_mask'])\n            }\n        else:\n            label = self.labels[idx]\n            return {\n                'input_ids': torch.tensor(encoding['input_ids']), \n                'attention_mask': torch.tensor(encoding['attention_mask']), \n                'labels': torch.tensor(label)\n            }\n\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Create the datasets\ntrain_dataset = DisasterTweetDataset(train_df, tokenizer)\nval_dataset = DisasterTweetDataset(val_df, tokenizer)\ntest_dataset = DisasterTweetDataset(test_df, tokenizer, is_test=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)\n\nclass DisasterTweetClassifier(pl.LightningModule):\n    def __init__(self, lr):\n        super().__init__()\n        num_labels = 2  # Binary classification: target is either 0 or 1\n        self.model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n        self.loss = nn.CrossEntropyLoss()\n        self.lr = lr  # set learning rate\n        self.val_f1 = F1Score(task='binary', num_classes=num_labels, average='macro')\n        self.train_accuracy = Accuracy(task='binary', num_classes=num_labels)\n        self.val_accuracy = Accuracy(task='binary', num_classes=num_labels)\n        self.test_outputs = []  # Initialize an empty list to store test outputs\n        self.train_losses = []  # Initialize an empty list to store training losses\n        self.train_f1_scores = []  # Initialize an empty list to store training F1 scores\n        \n    def forward(self, inputs):\n        outputs = self.model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask']).logits\n        return outputs\n\n    def training_step(self, batch, batch_idx):\n        inputs = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}\n        labels = batch['labels']\n        outputs = self.forward(inputs)\n        loss = self.loss(outputs, labels)\n        preds = torch.argmax(outputs, dim=1)\n        f1_score = self.val_f1(preds, labels).item()  # Compute F1 score for the batch\n        train_acc = self.train_accuracy(preds, labels)\n        self.log('train_acc', train_acc, prog_bar=True)\n        self.train_f1_scores.append(f1_score)  # Append F1 score for this batch\n        self.log('train_loss', loss, prog_bar=True)\n        self.log('f1', f1_score, prog_bar=True)\n        return {'loss': loss}\n\n    def on_train_epoch_end(self):\n        if self.train_losses:  # Check if list is not empty\n            avg_loss = torch.stack(self.train_losses).mean()  # Compute the average loss\n            self.log('avg_train_loss', avg_loss)\n        if self.train_f1_scores:  # Check if list is not empty\n            avg_f1 = sum(self.train_f1_scores) / len(self.train_f1_scores)  # Compute the average F1 score\n            self.log('avg_train_f1', torch.tensor(avg_f1))  # Log the average F1 score\n        self.train_losses = []  # Clear the list for the next epoch\n        self.train_f1_scores = []  # Clear the F1 scores for the next epoch\n\n    def configure_optimizers(self):\n        optimizer = Adam(self.parameters(), lr=self.lr, weight_decay=0.01)\n        total_steps = (len(train_dataset) // TRAIN_BATCH_SIZE) * NUM_EPOCHS\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=150, num_training_steps=total_steps)\n        return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]\n\n    def validation_step(self, batch, batch_idx):\n        inputs = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}\n        labels = batch['labels']\n        outputs = self.forward(inputs)\n        loss = self.loss(outputs, labels)\n        preds = torch.argmax(outputs, dim=1)\n        f1_value = self.val_f1(preds, labels)  # Compute F1 value\n        val_acc = self.val_accuracy(preds, labels)\n        # Log validation loss, accuracy and F1 score\n        self.log('val_acc', val_acc, prog_bar=True)\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_f1', f1_value, prog_bar=True)  # Log the computed F1 value\n        return {'val_loss': loss}\n\n    def test_step(self, batch, batch_idx):\n        inputs = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}\n        outputs = self.forward(inputs)\n        preds = torch.argmax(outputs, dim=1)\n        self.test_outputs.append({'logits': outputs})  # Append logits to test_outputs\n        return {'logits': outputs}\n\n    def on_test_epoch_end(self):\n        all_logits = torch.cat([x['logits'] for x in self.test_outputs], dim=0)\n        all_preds = torch.argmax(all_logits, dim=1)\n        ids = test_df['id'].values  # Get the ids from the test DataFrame\n        submission_df = pd.DataFrame({'id': ids, 'target': all_preds.cpu().numpy()})  # Create a DataFrame for submission\n        submission_df.to_csv('submission.csv', index=False)  # Save the DataFrame to a CSV file\n        self.test_outputs = []  # Clear the test_outputs list for future test runs\n        \n    def test_step_end(self, batch_parts):\n        if not hasattr(self, 'test_outputs'):\n            self.test_outputs = []\n        self.test_outputs.append(batch_parts)\n        \n# Initialize the model\nprint(\"Initializing model...\")\nmodel = DisasterTweetClassifier(lr=LEARNING_RATE)\n\n# Initialize Trainer\nprint(\"Initializing trainer...\")\ntrainer = pl.Trainer(\n    max_epochs=NUM_EPOCHS,\n    accelerator=\"auto\",\n)\n\n#Wrap the training phase with try-except for error handling\ntry:\n    # Train the model\n    print(\"Training the model...\")\n    #num_workers=1\n    trainer.fit(model, DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=1), DataLoader(val_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=1))\nexcept RuntimeError as e:\n    if \"out of memory\" in str(e):\n        print(\"ERROR: Out of memory\")\n        # Add code here to handle OOM specifically, like freeing up resources or reducing batch size\n    else:\n        print(\"An unexpected error occurred during training.\")\n        print(str(e))\nexcept Exception as e:\n    print(\"An unexpected error occurred during training.\")\n    print(str(e))\n\nprint(\"Testing the model...\")\ntrainer.test(model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2023-10-05T23:06:31.903860Z","iopub.execute_input":"2023-10-05T23:06:31.904240Z","iopub.status.idle":"2023-10-05T23:08:25.469389Z","shell.execute_reply.started":"2023-10-05T23:06:31.904202Z","shell.execute_reply":"2023-10-05T23:08:25.468394Z"},"trusted":true},"execution_count":null,"outputs":[]}]}